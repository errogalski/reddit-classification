{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b2081b8-42f8-4e57-b30a-3056f987055d",
   "metadata": {},
   "source": [
    "# API Function # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e07585b-d826-4fde-983c-f9d8b4968978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85bd8e5-5805-4b09-a9d6-94a7ad2bf8bf",
   "metadata": {},
   "source": [
    "## Function that pulls down posts from a specific subreddit ## \n",
    "- Subreddit: which subreddit to pull posts from\n",
    "- num_posts: the number of posts to pull down\n",
    "- date: when you would like to pull posts from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60589d60-eb6b-450d-aa8f-171897169136",
   "metadata": {},
   "outputs": [],
   "source": [
    "url =  'https://api.pushshift.io/reddit/search/submission'\n",
    "#comment_url = 'https://api.pushshift.io/reddit/search/comment'\n",
    "def get_posts(subreddit, num_posts, date):\n",
    "    initial_timestamp = math.floor(date.timestamp())\n",
    "    if num_posts > 100:\n",
    "        size = 100\n",
    "    else:\n",
    "        size = num_posts\n",
    "    \n",
    "    params = { 'subreddit': subreddit, 'size': size, 'before': initial_timestamp }\n",
    "    print('Fetching first batch of posts from r/'+subreddit)\n",
    "    results = requests.get(url, params)\n",
    "    data = results.json()\n",
    "    posts = data['data']\n",
    "    remaining_posts = num_posts-len(posts)\n",
    "    while remaining_posts > 0:\n",
    "        time.sleep(1) # 1 second sleep seems to be sufficient to avoid 429 responses\n",
    "        if remaining_posts > 100:\n",
    "            size = 100\n",
    "        else:\n",
    "            size = remaining_posts\n",
    "        last_post_time = posts[len(posts)-1]['created_utc']\n",
    "        \n",
    "        params['before'] = last_post_time\n",
    "        params['size'] = size\n",
    "        \n",
    "        print('Fetching posts '+str(len(posts)+1)+'-'+str(len(posts)+size)+' from r/'+subreddit)\n",
    "        results = requests.get(url, params)\n",
    "        try:\n",
    "            data = results.json()\n",
    "        except:\n",
    "            print(results)\n",
    "        loop_posts = data['data']\n",
    "        for i in range(len(loop_posts)):\n",
    "            posts.append(loop_posts[i])\n",
    "        remaining_posts = num_posts-len(posts)\n",
    "    print('Done fetching posts from r/'+subreddit)    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe269dc-c176-4545-9593-4dfcec0b06c4",
   "metadata": {},
   "source": [
    "## Pull Down Posts: ##\n",
    "- can specify from which subreddit, how many posts, and from which point in time\n",
    "- I chose 1500 posts each and used current posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63da024-75af-4e2f-9c47-154c723854a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching first batch of posts from r/baseball\n",
      "Fetching posts 101-200 from r/baseball\n",
      "Fetching posts 201-300 from r/baseball\n",
      "Fetching posts 301-400 from r/baseball\n",
      "Fetching posts 401-500 from r/baseball\n",
      "Fetching posts 501-600 from r/baseball\n",
      "Fetching posts 601-700 from r/baseball\n",
      "Fetching posts 701-800 from r/baseball\n",
      "Fetching posts 801-900 from r/baseball\n",
      "Fetching posts 901-1000 from r/baseball\n",
      "Fetching posts 1001-1100 from r/baseball\n",
      "Fetching posts 1101-1200 from r/baseball\n",
      "Fetching posts 1201-1300 from r/baseball\n",
      "Fetching posts 1301-1400 from r/baseball\n",
      "Fetching posts 1401-1500 from r/baseball\n",
      "Done fetching posts from r/baseball\n",
      "Fetching first batch of posts from r/hockey\n",
      "Fetching posts 101-200 from r/hockey\n",
      "Fetching posts 201-300 from r/hockey\n",
      "Fetching posts 301-400 from r/hockey\n",
      "Fetching posts 401-500 from r/hockey\n",
      "Fetching posts 501-600 from r/hockey\n",
      "Fetching posts 601-700 from r/hockey\n",
      "Fetching posts 701-800 from r/hockey\n",
      "Fetching posts 801-900 from r/hockey\n",
      "Fetching posts 901-1000 from r/hockey\n",
      "Fetching posts 1001-1100 from r/hockey\n",
      "Fetching posts 1101-1200 from r/hockey\n",
      "Fetching posts 1199-1298 from r/hockey\n",
      "Fetching posts 1297-1396 from r/hockey\n",
      "Fetching posts 1396-1495 from r/hockey\n",
      "Fetching posts 1496-1500 from r/hockey\n",
      "Done fetching posts from r/hockey\n",
      "Fetching first batch of posts from r/nba\n",
      "Fetching posts 100-199 from r/nba\n",
      "Fetching posts 199-298 from r/nba\n",
      "Fetching posts 299-398 from r/nba\n",
      "Fetching posts 399-498 from r/nba\n",
      "Fetching posts 499-598 from r/nba\n",
      "Fetching posts 599-698 from r/nba\n",
      "Fetching posts 699-798 from r/nba\n",
      "Fetching posts 799-898 from r/nba\n",
      "Fetching posts 898-997 from r/nba\n",
      "Fetching posts 998-1097 from r/nba\n",
      "Fetching posts 1098-1197 from r/nba\n",
      "Fetching posts 1198-1297 from r/nba\n",
      "Fetching posts 1298-1397 from r/nba\n",
      "Fetching posts 1398-1497 from r/nba\n",
      "Fetching posts 1498-1500 from r/nba\n",
      "Done fetching posts from r/nba\n",
      "Fetching first batch of posts from r/nfl\n",
      "Fetching posts 101-200 from r/nfl\n",
      "Fetching posts 201-300 from r/nfl\n",
      "Fetching posts 301-400 from r/nfl\n",
      "Fetching posts 401-500 from r/nfl\n",
      "Fetching posts 501-600 from r/nfl\n",
      "Fetching posts 601-700 from r/nfl\n",
      "Fetching posts 701-800 from r/nfl\n",
      "Fetching posts 801-900 from r/nfl\n",
      "Fetching posts 901-1000 from r/nfl\n",
      "Fetching posts 1001-1100 from r/nfl\n",
      "Fetching posts 1101-1200 from r/nfl\n",
      "Fetching posts 1201-1300 from r/nfl\n",
      "Fetching posts 1301-1400 from r/nfl\n",
      "Fetching posts 1401-1500 from r/nfl\n",
      "Done fetching posts from r/nfl\n",
      "Fetching first batch of posts from r/soccer\n",
      "Fetching posts 101-200 from r/soccer\n",
      "Fetching posts 201-300 from r/soccer\n",
      "Fetching posts 301-400 from r/soccer\n",
      "Fetching posts 401-500 from r/soccer\n",
      "Fetching posts 501-600 from r/soccer\n",
      "Fetching posts 601-700 from r/soccer\n",
      "Fetching posts 701-800 from r/soccer\n",
      "Fetching posts 801-900 from r/soccer\n",
      "Fetching posts 901-1000 from r/soccer\n",
      "Fetching posts 1001-1100 from r/soccer\n",
      "Fetching posts 1101-1200 from r/soccer\n",
      "Fetching posts 1201-1300 from r/soccer\n",
      "Fetching posts 1301-1400 from r/soccer\n",
      "Fetching posts 1401-1500 from r/soccer\n",
      "Done fetching posts from r/soccer\n"
     ]
    }
   ],
   "source": [
    "baseball_posts = get_posts('baseball', 1500, datetime.datetime.now())\n",
    "hockey_posts = get_posts('hockey', 1500, datetime.datetime.now())\n",
    "nba_posts = get_posts('nba', 1500, datetime.datetime.now())\n",
    "nfl_posts = get_posts('nfl', 1500, datetime.datetime.now())\n",
    "soccer_posts = get_posts('soccer', 1500, datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a2ff8-2a71-4632-8a08-d936ba538f00",
   "metadata": {},
   "source": [
    "## Create and Export DataFrames: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c23009-0548-4c8e-8a71-56dcc1ff8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_df = pd.DataFrame(baseball_posts)\n",
    "hockey_df = pd.DataFrame(hockey_posts)\n",
    "nba_df = pd.DataFrame(nba_posts)\n",
    "nfl_df = pd.DataFrame(nfl_posts)\n",
    "soccer_df = pd.DataFrame(soccer_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93f6843f-052c-4f14-b4d6-30cdacea183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_df.to_csv('./data/nba.csv')\n",
    "baseball_df.to_csv('./data/baseball.csv')\n",
    "nfl_df.to_csv('./data/nfl.csv')\n",
    "soccer_df.to_csv('./data/soccer.csv')\n",
    "hockey_df.to_csv('./data/hockey.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
